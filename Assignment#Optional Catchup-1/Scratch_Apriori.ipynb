{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scratch_Apriori.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9txWGFU5SIsv"
      },
      "outputs": [],
      "source": [
        "##Functions for the algorithm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(file_loc='breadbasket.csv'):\n",
        "    trans = dict()\n",
        "    with open(file_loc) as f:\n",
        "        filedata = csv.reader(f, delimiter=',')\n",
        "        count = 0\n",
        "        for line in filedata:\n",
        "            count += 1\n",
        "            trans[count] = list(set(line))\n",
        "    return trans"
      ],
      "metadata": {
        "id": "1TMZngrsVIkX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to get frequent one itemset\n",
        "def frequent_one_item(Transaction,min_support):\n",
        "    candidate1 = {}\n",
        "\n",
        "    for i in range(0,len(Transaction)):\n",
        "        for j in range(0,len(Transaction[i])):\n",
        "            if Transaction[i][j] not in candidate1:\n",
        "                candidate1[Transaction[i][j]] = 1\n",
        "            else:\n",
        "                candidate1[Transaction[i][j]] += 1\n",
        "\n",
        "    frequentitem1 = []                      #to get frequent 1 itemsets with minimum support count\n",
        "    for value in candidate1:\n",
        "        if candidate1[value] >= min_support:\n",
        "            frequentitem1 = frequentitem1 + [[value]]\n",
        "            Frequent_items_value[tuple(value)] = candidate1[value]\n",
        "\n",
        "    return frequentitem1"
      ],
      "metadata": {
        "id": "znYid-9hWifz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#class of Hash node\n",
        "class Hash_node:\n",
        "    def __init__(self):\n",
        "        self.children = {}           #pointer to its children\n",
        "        self.Leaf_status = True      #to know the status whether current node is leaf or not\n",
        "        self.bucket = {}             #contains itemsets in bucket"
      ],
      "metadata": {
        "id": "cNL2R79vWjnY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#class of constructing and getting hashtree\n",
        "class HashTree:\n",
        "    # class constructor\n",
        "    def __init__(self, max_leaf_count, max_child_count):\n",
        "        self.root = Hash_node()\n",
        "        self.max_leaf_count = max_leaf_count\n",
        "        self.max_child_count = max_child_count\n",
        "        self.frequent_itemsets = []\n",
        "\n",
        "    # function to recursive insertion to make hashtree\n",
        "    def recursively_insert(self, node, itemset, index, count):\n",
        "        if index == len(itemset):\n",
        "            if itemset in node.bucket:\n",
        "                node.bucket[itemset] += count\n",
        "            else:\n",
        "                node.bucket[itemset] = count\n",
        "            return\n",
        "\n",
        "        if node.Leaf_status:                             #if node is leaf\n",
        "            if itemset in node.bucket:\n",
        "                node.bucket[itemset] += count\n",
        "            else:\n",
        "                node.bucket[itemset] = count\n",
        "            if len(node.bucket) == self.max_leaf_count:  #if bucket capacity increases\n",
        "                for old_itemset, old_count in node.bucket.items():\n",
        "\n",
        "                    hash_key = self.hash_function(old_itemset[index])  #do hashing on next index\n",
        "                    if hash_key not in node.children:\n",
        "                        node.children[hash_key] = Hash_node()\n",
        "                    self.recursively_insert(node.children[hash_key], old_itemset, index + 1, old_count)\n",
        "                #since no more requirement of this bucket\n",
        "                del node.bucket\n",
        "                node.Leaf_status = False\n",
        "        else:                                            #if node is not leaf\n",
        "            hash_key = self.hash_function(itemset[index])\n",
        "            if hash_key not in node.children:\n",
        "                node.children[hash_key] = Hash_node()\n",
        "            self.recursively_insert(node.children[hash_key], itemset, index + 1, count)\n",
        "\n",
        "    def insert(self, itemset):\n",
        "        itemset = tuple(itemset)\n",
        "        self.recursively_insert(self.root, itemset, 0, 0)\n",
        "\n",
        "    # to add support to candidate itemsets. Transverse the Tree and find the bucket in which this itemset is present.\n",
        "    def add_support(self, itemset):\n",
        "        Transverse_HNode = self.root\n",
        "        itemset = tuple(itemset)\n",
        "        index = 0\n",
        "        while True:\n",
        "            if Transverse_HNode.Leaf_status:\n",
        "                if itemset in Transverse_HNode.bucket:    #found the itemset in this bucket\n",
        "                    Transverse_HNode.bucket[itemset] += 1 #increment the count of this itemset.\n",
        "                break\n",
        "            hash_key = self.hash_function(itemset[index])\n",
        "            if hash_key in Transverse_HNode.children:\n",
        "                Transverse_HNode = Transverse_HNode.children[hash_key]\n",
        "            else:\n",
        "                break\n",
        "            index += 1\n",
        "\n",
        "    # to transverse the hashtree to get frequent itemsets with minimum support count\n",
        "    def get_frequent_itemsets(self, node, support_count,frequent_itemsets):\n",
        "        if node.Leaf_status:\n",
        "            for key, value in node.bucket.items():\n",
        "                if value >= support_count:                       #if it satisfies the condition\n",
        "                    frequent_itemsets.append(list(key))          #then add it to frequent itemsets.\n",
        "                    Frequent_items_value[key] = value\n",
        "            return\n",
        "\n",
        "        for child in node.children.values():\n",
        "            self.get_frequent_itemsets(child, support_count,frequent_itemsets)\n",
        "\n",
        "    # hash function for making HashTree\n",
        "    def hash_function(self, val):\n",
        "        return int(val) % self.max_child_count\n",
        "\n",
        "#To generate hash tree from candidate itemsets\n",
        "def generate_hash_tree(candidate_itemsets, max_leaf_count, max_child_count):\n",
        "    htree = HashTree(max_child_count, max_leaf_count)             #create instance of HashTree\n",
        "    for itemset in candidate_itemsets:\n",
        "        htree.insert(itemset)                                     #to insert itemset into Hashtree\n",
        "    return htree\n",
        "\n",
        "#to generate subsets of itemsets of size k\n",
        "def generate_k_subsets(dataset, length):\n",
        "    subsets = []\n",
        "    for itemset in dataset:\n",
        "        subsets.extend(map(list, itertools.combinations(itemset, length)))\n",
        "    return subsets\n",
        "\n",
        "def subset_generation(ck_data,l):\n",
        "    return map(list,set(itertools.combinations(ck_data,l)))\n",
        "\n",
        "#apriori generate function to generate ck\n",
        "def apriori_generate(dataset,k):\n",
        "    ck = []\n",
        "    #join step\n",
        "    lenlk = len(dataset)\n",
        "    for i in range(lenlk):\n",
        "        for j in range(i+1,lenlk):\n",
        "            L1 = list(dataset[i])[:k - 2]\n",
        "            L2 = list(dataset[j])[:k - 2]\n",
        "            if L1 == L2:\n",
        "                ck.append(sorted(list(set(dataset[i]) | set(dataset[j]))))\n",
        "\n",
        "    #prune step\n",
        "    final_ck = []\n",
        "    for candidate in ck:\n",
        "        all_subsets = list(subset_generation(set(candidate), k - 1))\n",
        "        found = True\n",
        "        for i in range(len(all_subsets)):\n",
        "            value = list(sorted(all_subsets[i]))\n",
        "            if value not in dataset:\n",
        "                found = False\n",
        "        if found == True:\n",
        "            final_ck.append(candidate)\n",
        "\n",
        "    return ck,final_ck\n",
        "\n",
        "def generateL(ck,min_support):\n",
        "    support_ck = {}\n",
        "    for val in Transaction1:\n",
        "        for val1 in ck:\n",
        "            value = set(val)\n",
        "            value1 = set(val1)\n",
        "\n",
        "            if value1.issubset(value):\n",
        "                if tuple(val1) not in support_ck:\n",
        "                    support_ck[tuple(val1)] = 1\n",
        "                else:\n",
        "                    support_ck[tuple(val1)] += 1\n",
        "    frequent_item = []\n",
        "    for item_set in support_ck:\n",
        "        if support_ck[item_set] >= min_support:\n",
        "            frequent_item.append(sorted(list(item_set)))\n",
        "            Frequent_items_value[item_set] = support_ck[item_set]\n",
        "\n",
        "    return frequent_item"
      ],
      "metadata": {
        "id": "qpk3zvC2WtPm"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def association_rules(items_grater_then_min_support):\n",
        "    rules = []\n",
        "    dict_rules = {}\n",
        "    for i in items_grater_then_min_support:\n",
        "        dict_rules = {}\n",
        "        if type(i) != type(str()):\n",
        "            i = list(i)\n",
        "            temp_i = i[:]\n",
        "            for j in range(len(i)):\n",
        "                k = temp_i[j]\n",
        "                del temp_i[j]\n",
        "                dict_rules[k] = temp_i\n",
        "                temp_i = i[:]\n",
        "        rules.append(dict_rules)\n",
        "    temp = []\n",
        "    for i in rules:\n",
        "        for j in i.items():\n",
        "            if type(j[1]) != type(str()):\n",
        "                temp.append({tuple(j[1])[0]: j[0]})\n",
        "            else:\n",
        "                temp.append({j[1]: j[0]})\n",
        "    rules.extend(temp)\n",
        "    return rules\n",
        "\n",
        "def confidence(associations, d, min_confidence):\n",
        "    ans = {}\n",
        "    for i in associations:\n",
        "        for j in i.items():\n",
        "            if type(j[0]) == type(str()):\n",
        "                left = {j[0]}\n",
        "            else:\n",
        "                left = set(j[0])\n",
        "            if type(j[1]) == type(str()):\n",
        "                right = {j[1]}\n",
        "            else:\n",
        "                right = set(j[1])\n",
        "            for k in d:\n",
        "                if type(k) != type(str()):\n",
        "                    if left.union(right) - set(k) == set():\n",
        "                        up = d[k]\n",
        "                    if len(right) == len(set(k)) and right - set(k) == set():\n",
        "                            down = d[k]\n",
        "                else:\n",
        "                    if len(right) >= len({k}):\n",
        "                        if right - {k} == set():\n",
        "                            down = d[k]\n",
        "                    elif len(right) <= len({k}):\n",
        "                        if {k} - right == set():\n",
        "                            down = d[k]\n",
        "            if up/down >= min_confidence:\n",
        "                ans[tuple(left)[0]] = right, up/down, up, down\n",
        "    print(ans)   "
      ],
      "metadata": {
        "id": "wQSq5PMJmOO-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## main apriori algorithm function"
      ],
      "metadata": {
        "id": "UKcRevZyWtHl"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from itertools import combinations"
      ],
      "metadata": {
        "id": "mX-53f1sXOFB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def frequence(items_lst, trans, check=False):\n",
        "    items_counts = dict()\n",
        "    for i in items_lst:\n",
        "        temp_i = {i}\n",
        "        if check:\n",
        "            temp_i = set(i)\n",
        "        for j in trans.items():\n",
        "            if temp_i.issubset(set(j[1])):\n",
        "                if i in items_counts:\n",
        "                    items_counts[i] += 1\n",
        "                else:\n",
        "                    items_counts[i] = 1\n",
        "    return items_counts\n",
        "    \n",
        "\n",
        "def support(items_counts, trans):\n",
        "    support = dict()\n",
        "    total_trans = len(trans)\n",
        "    for i in items_counts:\n",
        "        support[i] = items_counts[i]/total_trans\n",
        "    return support"
      ],
      "metadata": {
        "id": "68Oxm3M9mKHh"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(min_support, min_confidence, file_loc):\n",
        "    \n",
        "    trans = read_data()\n",
        "    number_of_trans = [len(i) for i in trans.values()]\n",
        "    items_lst = set()\n",
        "    \n",
        "    itemcount_track = list()    \n",
        "    \n",
        "    for i in trans.values():\n",
        "        for j in i:\n",
        "            items_lst.add(j)\n",
        "    \n",
        "    store_item_lst = list(items_lst)[:]\n",
        "    items_grater_then_min_support = list()\n",
        "    items_counts = frequence(items_lst, trans)\n",
        "    itemcount_track.append(items_counts)\n",
        "    items_grater_then_min_support.append({j[0]:j[1] for j in support(items_counts, trans).items() if j[1]>min_support})\n",
        "    \n",
        "    for i in range(2, max(number_of_trans)+1):\n",
        "        item_list = combinations(items_lst, i)\n",
        "        items_counts = frequence(item_list, trans, check=True)\n",
        "        itemcount_track.append(items_counts)\n",
        "        if list({j[0]:j[1] for j in support(items_counts, trans).items() if j[1]>min_support}.keys()) != []:\n",
        "            items_grater_then_min_support.append({j[0]:j[1] for j in support(items_counts, trans).items() if j[1]>min_support})\n",
        "        \n",
        "    d = {}\n",
        "    \n",
        "    {d.update(i) for i in itemcount_track}\n",
        "    \n",
        "    associations = association_rules(items_grater_then_min_support[len(items_grater_then_min_support)-1])\n",
        "    associations_grater_then_confidene = confidence(associations, d, min_confidence)\n",
        "    "
      ],
      "metadata": {
        "id": "YiRBzZe6mLMl"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(0.02, 0.8, 'breadbasket.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PEB2JXLmb_w",
        "outputId": "a2b13f6d-e1e0-49df-a70a-f40cd0811f0d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'13': ({'Basket'}, 1.0, 1, 1), '30-10-2016 10:31': ({'13', 'morning', 'weekend', 'Basket'}, 1.0, 1, 1), 'morning': ({'30-10-2016 10:05'}, 1.0, 2, 2), 'weekend': ({'Tea', '30-10-2016 10:19', 'morning', '7'}, 1.0, 1, 1), '10': ({'30-10-2016 10:25', 'Scandinavian', 'morning', 'weekend'}, 1.0, 1, 1), '30-10-2016 10:25': ({'10', 'morning', 'Scandinavian', 'weekend'}, 1.0, 1, 1), '3': ({'Hot chocolate'}, 1.0, 1, 1), '30-10-2016 10:07': ({'morning', '3', 'Hot chocolate', 'weekend'}, 1.0, 1, 1), '12': ({'Tartine'}, 1.0, 1, 1), '30-10-2016 10:30': ({'Tea', 'morning', 'weekend', '12'}, 1.0, 1, 1), 'Transaction': ({'weekday_weekend'}, 1.0, 1, 1), 'Item': ({'Transaction'}, 1.0, 1, 1), 'date_time': ({'Transaction', 'period_day', 'Item', 'weekday_weekend'}, 1.0, 1, 1), 'period_day': ({'Transaction', 'weekday_weekend', 'Item', 'date_time'}, 1.0, 1, 1), 'weekday_weekend': ({'Transaction', 'period_day', 'Item', 'date_time'}, 1.0, 1, 1), '30-10-2016 10:13': ({'5', 'morning', 'weekend', 'Pastry'}, 1.0, 1, 1), '5': ({'morning', 'weekend', 'Pastry', '30-10-2016 10:13'}, 1.0, 1, 1), '30-10-2016 10:19': ({'Tea', 'morning', '7', 'weekend'}, 1.0, 1, 1), '7': ({'Tea', '30-10-2016 10:19', 'morning', 'weekend'}, 1.0, 1, 1), '30-10-2016 09:58': ({'1'}, 1.0, 1, 1), 'Bread': ({'30-10-2016 09:58'}, 1.0, 1, 1), '1': ({'Bread', '30-10-2016 09:58', 'weekend', 'morning'}, 1.0, 1, 1), '15': ({'Mineral water'}, 1.0, 1, 1), '30-10-2016 10:34': ({'15'}, 1.0, 1, 1), 'Mineral water': ({'morning', '30-10-2016 10:34', '15', 'weekend'}, 1.0, 1, 1), '30-10-2016 10:05': ({'2'}, 1.0, 2, 2), '2': ({'30-10-2016 10:05', 'Scandinavian', 'morning', 'weekend'}, 1.0, 2, 2), 'Scandinavian': ({'30-10-2016 10:05', 'morning', '2', 'weekend'}, 1.0, 2, 2), '30-10-2016 10:27': ({'Bread', '11', 'morning', 'weekend'}, 1.0, 2, 2), '11': ({'30-10-2016 10:27', 'Bread', 'morning', 'weekend'}, 1.0, 2, 2), '30-10-2016 10:32': ({'14', 'morning', 'weekend', 'Pastry'}, 1.0, 1, 1), '14': ({'morning', '30-10-2016 10:32', 'Pastry', 'weekend'}, 1.0, 1, 1), '30-10-2016 10:16': ({'6', 'morning', 'weekend', 'Pastry'}, 1.0, 1, 1), '6': ({'morning', '30-10-2016 10:16', 'weekend', 'Pastry'}, 1.0, 1, 1), '8': ({'30-10-2016 10:20', 'morning', 'weekend', 'Pastry'}, 1.0, 1, 1), '30-10-2016 10:20': ({'8', 'morning', 'weekend', 'Pastry'}, 1.0, 1, 1), '30-10-2016 10:21': ({'9', 'Muffin', 'morning', 'weekend'}, 1.0, 1, 1), '9': ({'Muffin', '30-10-2016 10:21', 'morning', 'weekend'}, 1.0, 1, 1), 'Muffin': ({'4'}, 1.0, 1, 1), '30-10-2016 10:08': ({'Muffin', 'morning', 'weekend', '4'}, 1.0, 1, 1), '4': ({'Muffin', 'morning', '30-10-2016 10:08', 'weekend'}, 1.0, 1, 1)}\n"
          ]
        }
      ]
    }
  ]
}