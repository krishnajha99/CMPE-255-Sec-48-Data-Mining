{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "colab": {
      "name": "State_Of_Art_Approximate_Nearest_Neighbor_Search.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvwevIN8z3lF"
      },
      "source": [
        "# State of art libraries for Approximate nearest neighbor search for my favorite LightFM data set. \n",
        "The LightFM's built-in Dataset class to build an interaction dataset from raw data.  https://archive.org/details/stackexchange\n",
        "\n",
        "The goal is to demonstrate how to use different ANN Search works \n",
        "* LSH\n",
        "* Exhaustive search\n",
        "* Product quantization\n",
        "* Trees and graphs\n",
        "* HNSW\n",
        "\n",
        "Data Set Dependencies:\n",
        "* Annoy \n",
        "* NMSLIB\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BT_AE8f4z3lI",
        "outputId": "4e7fd20f-c77f-47eb-82d9-536325b8064a"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import time\n",
        "!pip install lightFM\n",
        "from lightfm.datasets import fetch_stackexchange\n",
        "\n",
        "#movielens = fetch_movielens()\n",
        "data = fetch_stackexchange('crossvalidated',\n",
        "                           test_set_fraction=0.1,\n",
        "                           indicator_features=False,\n",
        "                           tag_features=True)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightFM in /usr/local/lib/python3.7/dist-packages (1.16)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from lightFM) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from lightFM) (1.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lightFM) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from lightFM) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->lightFM) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->lightFM) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->lightFM) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->lightFM) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightFM) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightFM) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCYeba_sz3lK",
        "outputId": "52078fcb-3d62-4fcf-e572-5a6eae658d40"
      },
      "source": [
        "for key, value in data.items():\n",
        "    print(key, type(value), value.shape)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train <class 'scipy.sparse.coo.coo_matrix'> (3213, 72360)\n",
            "test <class 'scipy.sparse.coo.coo_matrix'> (3213, 72360)\n",
            "item_features <class 'scipy.sparse.csr.csr_matrix'> (72360, 1246)\n",
            "item_feature_labels <class 'numpy.ndarray'> (1246,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "VgJtPO04z3lL"
      },
      "source": [
        "stackexchange = fetch_stackexchange('crossvalidated',indicator_features=True,tag_features=True)\n",
        "train = data['train']\n",
        "test = data['test']"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OINaZbrFz3lL",
        "outputId": "0983e789-20b5-43f4-a305-7cb890248055"
      },
      "source": [
        "from lightfm import LightFM\n",
        "\n",
        "# Set the number of threads; you can increase this\n",
        "# ify you have more physical cores available.\n",
        "NUM_THREADS = 2\n",
        "NUM_COMPONENTS = 30\n",
        "NUM_EPOCHS = 3\n",
        "ITEM_ALPHA = 1e-6\n",
        "\n",
        "# Let's fit a WARP model: these generally have the best performance.\n",
        "model = LightFM(loss='warp',\n",
        "                item_alpha=ITEM_ALPHA,\n",
        "               no_components=NUM_COMPONENTS)\n",
        "\n",
        "# Run 3 epochs and time it.\n",
        "%time model = model.fit(train, epochs=NUM_EPOCHS, num_threads=NUM_THREADS)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 710 ms, sys: 6.78 ms, total: 717 ms\n",
            "Wall time: 392 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3B-6Dumz3lM"
      },
      "source": [
        "As a means of sanity checking, let's calculate the model's AUC on the training set first. If it's reasonably high, we can be sure that the model is not doing anything stupid and is fitting the training data well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4l_uttcz3lM",
        "outputId": "cf0209eb-d72e-458b-c700-f8a446ba1c2f"
      },
      "source": [
        "# Import the evaluation routines\n",
        "from lightfm.evaluation import auc_score\n",
        "\n",
        "# Compute and print the AUC score\n",
        "train_auc = auc_score(model, train, num_threads=NUM_THREADS).mean()\n",
        "print('Collaborative filtering train AUC: %s' % train_auc)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collaborative filtering train AUC: 0.8891438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWMzEvTdz3lN"
      },
      "source": [
        "Fantastic, the model is fitting the training set well. But what about the test set.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQBsfvpSz3lN",
        "outputId": "05fd0a72-94ea-49f3-ecc2-2d52fbfd9883"
      },
      "source": [
        "# We pass in the train interactions to exclude them from predictions.\n",
        "test_auc = auc_score(model, test, train_interactions=train, num_threads=NUM_THREADS).mean()\n",
        "print('Collaborative filtering test AUC: %s' % test_auc)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collaborative filtering test AUC: 0.41064462\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF31FHUGz3lO"
      },
      "source": [
        " Re-evaluating the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkoMbNKez3lO",
        "outputId": "3deb01d2-8a59-4000-d83d-8bd6c6e8c8de"
      },
      "source": [
        "# Set biases to zero\n",
        "model.item_biases *= 0.0\n",
        "\n",
        "test_auc = auc_score(model, test, train_interactions=train, num_threads=NUM_THREADS).mean()\n",
        "print('Collaborative filtering test AUC: %s' % test_auc)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collaborative filtering test AUC: 0.5331501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q94AdDHkz3lQ",
        "outputId": "024525e5-cca0-4928-919e-20725d9d578c"
      },
      "source": [
        "!pip install nmslib\n",
        "import nmslib\n",
        "\n",
        "# initialize a new nmslib index, using a HNSW index on Cosine Similarity\n",
        "nms_idx = nmslib.init(method='hnsw', space='cosinesimil')\n",
        "nms_idx.addDataPointBatch(item_embeddings)\n",
        "nms_idx.createIndex(print_progress=True)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nmslib in /usr/local/lib/python3.7/dist-packages (2.1.1)\n",
            "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from nmslib) (1.19.5)\n",
            "Requirement already satisfied: pybind11<2.6.2 in /usr/local/lib/python3.7/dist-packages (from nmslib) (2.6.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from nmslib) (5.4.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2ftG4bQAnWV"
      },
      "source": [
        "model = LightFM(learning_rate=0.05, loss='warp', no_components=64, item_alpha=0.001)\n",
        "model.fit_partial(train, item_features=stackexchange['item_features'], epochs=20 )\n",
        "\n",
        "item_vectors = stackexchange['item_features'] * model.item_embeddings"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU-Q2rVuA0wr"
      },
      "source": [
        "# Print the Dataset stackexchange"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhoNwZZ5A7PX",
        "outputId": "8513bb47-d2c1-4d76-871e-37119237bed5"
      },
      "source": [
        "print(stackexchange['item_features'])"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 0)\t1.0\n",
            "  (0, 72360)\t1.0\n",
            "  (0, 72361)\t1.0\n",
            "  (0, 72362)\t1.0\n",
            "  (1, 1)\t1.0\n",
            "  (1, 72363)\t1.0\n",
            "  (1, 72364)\t1.0\n",
            "  (2, 2)\t1.0\n",
            "  (2, 72365)\t1.0\n",
            "  (2, 72366)\t1.0\n",
            "  (3, 3)\t1.0\n",
            "  (3, 72363)\t1.0\n",
            "  (3, 72367)\t1.0\n",
            "  (4, 4)\t1.0\n",
            "  (4, 72368)\t1.0\n",
            "  (5, 5)\t1.0\n",
            "  (5, 72369)\t1.0\n",
            "  (5, 72370)\t1.0\n",
            "  (5, 72371)\t1.0\n",
            "  (5, 72372)\t1.0\n",
            "  (6, 6)\t1.0\n",
            "  (6, 72373)\t1.0\n",
            "  (7, 7)\t1.0\n",
            "  (7, 72374)\t1.0\n",
            "  (7, 72375)\t1.0\n",
            "  :\t:\n",
            "  (72354, 72837)\t1.0\n",
            "  (72354, 73124)\t1.0\n",
            "  (72354, 73164)\t1.0\n",
            "  (72355, 72355)\t1.0\n",
            "  (72355, 72436)\t1.0\n",
            "  (72355, 72548)\t1.0\n",
            "  (72355, 73090)\t1.0\n",
            "  (72356, 72356)\t1.0\n",
            "  (72356, 72440)\t1.0\n",
            "  (72356, 72513)\t1.0\n",
            "  (72356, 72731)\t1.0\n",
            "  (72356, 72796)\t1.0\n",
            "  (72356, 73057)\t1.0\n",
            "  (72357, 72357)\t1.0\n",
            "  (72357, 72404)\t1.0\n",
            "  (72357, 73399)\t1.0\n",
            "  (72358, 72358)\t1.0\n",
            "  (72358, 72406)\t1.0\n",
            "  (72358, 72411)\t1.0\n",
            "  (72358, 72747)\t1.0\n",
            "  (72358, 72920)\t1.0\n",
            "  (72359, 72359)\t1.0\n",
            "  (72359, 72503)\t1.0\n",
            "  (72359, 72507)\t1.0\n",
            "  (72359, 72616)\t1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRqgnCvkBGcV"
      },
      "source": [
        "Load pretrained models from a .pkl file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OIiHQrIBZo1",
        "outputId": "cb4175f0-264c-4559-9b98-c82009cf270e"
      },
      "source": [
        "import pickle\n",
        "with open('stackexchange.pickle', 'wb') as f:\n",
        "    pickle.dump({\"name\": stackexchange['item_feature_labels'], \"vector\": item_vectors,\"features\":stackexchange['item_features']}, f)\n",
        "print(stackexchange['item_feature_labels'])    \n"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['question_id:0' 'question_id:1' 'question_id:2' ... 'events'\n",
            " 'mutlivariate' 'sample-variance']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_1J1gBQBqeE"
      },
      "source": [
        "import pickle\n",
        "def load_data():\n",
        "    with open('stackexchange.pickle', 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return data\n",
        "data = load_data()"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeu1CSZHCGcx"
      },
      "source": [
        "Approximate based on the word2vec model for the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIVAfFqWF4FR",
        "outputId": "2673e7df-8df2-4635-8b69-caecf23c6bff"
      },
      "source": [
        "item_features = stackexchange['item_features']\n",
        "tag_labels = stackexchange['item_feature_labels']\n",
        "\n",
        "print('There are %s distinct tags, with values like %s.' % (item_features.shape[1], tag_labels[:3].tolist()))\n",
        "# Define a new model instance\n",
        "model = LightFM(loss='warp',\n",
        "                item_alpha=ITEM_ALPHA,\n",
        "                no_components=NUM_COMPONENTS)\n",
        "\n",
        "# Fit the hybrid model. Note that this time, we pass\n",
        "# in the item features matrix.\n",
        "model = model.fit(train,\n",
        "                item_features=item_features,\n",
        "                epochs=NUM_EPOCHS,\n",
        "                num_threads=NUM_THREADS)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 73606 distinct tags, with values like ['question_id:0', 'question_id:1', 'question_id:2'].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lpMZyTyCIF6",
        "outputId": "c278c5f8-96e5-42c3-f400-7a3d029e21ed"
      },
      "source": [
        "tag_labels = stackexchange['item_feature_labels']\n",
        "def get_similar_tags(model, tag_id):\n",
        "    # Define similarity as the cosine of the angle\n",
        "    # between the tag latent vectors\n",
        "    \n",
        "    # Normalize the vectors to unit length\n",
        "    tag_embeddings = (model.item_embeddings.T\n",
        "                      / np.linalg.norm(model.item_embeddings, axis=1)).T\n",
        "    \n",
        "    query_embedding = tag_embeddings[tag_id]\n",
        "    similarity = np.dot(tag_embeddings, query_embedding)\n",
        "    most_similar = np.argsort(-similarity)[1:4]\n",
        "    \n",
        "    return most_similar\n",
        "\n",
        "\n",
        "for tag in (u'bayesian', u'regression', u'survival'):\n",
        "    tag_id = tag_labels.tolist().index(tag)\n",
        "    print('Most similar tags for %s: %s' % (tag_labels[tag_id],\n",
        "                                            tag_labels[get_similar_tags(model, tag_id)]))"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar tags for bayesian: ['prior' 'question_id:23252' 'question_id:41988']\n",
            "Most similar tags for regression: ['question_id:38606' 'question_id:48040' 'question_id:20061']\n",
            "Most similar tags for survival: ['cox-model' 'question_id:15719' 'question_id:63613']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTw71cPGGnbq"
      },
      "source": [
        "Exhaustive search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jn5okzlOJMRy",
        "outputId": "ceb9dbe1-c8c4-4086-cfa7-75cecc46322e"
      },
      "source": [
        "import scipy.sparse\n",
        "import random\n",
        "import itertools\n",
        "cx = scipy.sparse.coo_matrix(stackexchange['item_features'])\n",
        "def using_coo(index):\n",
        "    arr=[]\n",
        "       \n",
        "    for i,j,v in zip(cx.row, cx.col, cx.data):\n",
        "      if i==index and i != j:\n",
        "        arr.append(data['name'][j])\n",
        "      elif i>index:\n",
        "        break\n",
        "    return arr\n",
        "      # print(j,data['name'][j])\n",
        "using_coo(0)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bayesian', 'prior', 'elicitation']"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkjJ_2ZkGpd9"
      },
      "source": [
        "class exSearch():\n",
        "    def __init__(self, vectors, labels):\n",
        "        self.vectors = vectors.astype('float32')\n",
        "        self.labels = labels\n",
        "        self.index = faiss.IndexFlatL2(vectors.shape[1])\n",
        "        self.index.add(self.vectors)\n",
        "    def get_similar_tagged_questions(self,vectors,k=12):\n",
        "        distances, indices = self.index.search(vectors, k) \n",
        "        similar_q_w_feature={}\n",
        "        q_features=[]\n",
        "        for i in indices[0]:\n",
        "          similar_q_w_feature[self.labels[i]]=using_coo(i)\n",
        "        return similar_q_w_feature\n",
        "       "
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zp7EZ7uGydz"
      },
      "source": [
        "# Exhaustive search for brute-force approach to combinatorial finding similar item in dataset "
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ot4yiF_zHm2A",
        "outputId": "20327de0-4fe3-4ea8-950b-a86a1c50fed4"
      },
      "source": [
        "!pip install faiss\n",
        "!apt install libomp-dev\n",
        "!python3 -m pip install --upgrade faiss-gpu==1.7.1\n",
        "import faiss\n",
        "index = exSearch(data[\"vector\"], data[\"name\"])\n",
        "index.get_similar_tagged_questions(data['vector'][0:4])"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss in /usr/local/lib/python3.7/dist-packages (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from faiss) (1.19.5)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libomp-dev is already the newest version (5.0.1-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
            "Requirement already satisfied: faiss-gpu==1.7.1 in /usr/local/lib/python3.7/dist-packages (1.7.1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question_id:0': ['bayesian', 'prior', 'elicitation'],\n",
              " 'question_id:15699': ['bayesian', 'prior', 'fitting'],\n",
              " 'question_id:18112': ['bayesian'],\n",
              " 'question_id:2001': ['bayesian', 'prior'],\n",
              " 'question_id:2490': ['bayesian', 'prior', 'distributions'],\n",
              " 'question_id:29552': ['bayesian', 'prior', 'elicitation'],\n",
              " 'question_id:31509': ['bayesian', 'distributions'],\n",
              " 'question_id:31875': ['bayesian', 'prior'],\n",
              " 'question_id:45542': ['bayesian', 'prior', 'elicitation'],\n",
              " 'question_id:4611': ['bayesian', 'maximum-likelihood'],\n",
              " 'question_id:8661': ['bayesian', 'prior'],\n",
              " 'question_id:9524': ['bayesian', 'prior']}"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFsgXRY1JXo2"
      },
      "source": [
        "LSH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-ats-0eJfIL"
      },
      "source": [
        "#LSH (Locality sensitive Hashing), an approach that enable us to search for approximate nearest neighbors for a given point in metric space\n",
        "class LSHashing():\n",
        "    def __init__(self, vectors, labels):\n",
        "        self.dimention = vectors.shape[1]\n",
        "        self.vectors = vectors.astype('float32')\n",
        "        self.labels = labels\n",
        "\n",
        "\n",
        "    def lsh_build(self, number_of_partition=8, search_in_x_partitions=2, subvector_size=8):\n",
        "        quantizer = faiss.IndexFlatL2(self.dimention)\n",
        "        self.index = faiss.IndexIVFPQ(quantizer, self.dimention, number_of_partition, search_in_x_partitions, subvector_size)\n",
        "        self.index.train(self.vectors)\n",
        "        self.index.add(self.vectors)\n",
        "\n",
        "    def get_similar_tagged_questions(self, vectors, k=12):\n",
        "        distances, indices = self.index.search(vectors, k) \n",
        "        similar_q_w_feature={}\n",
        "        q_features=[]\n",
        "        for i in indices[0]:\n",
        "          similar_q_w_feature[self.labels[i]]=using_coo(i)\n",
        "        return similar_q_w_feature"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rhXkqRoJr6R",
        "outputId": "7d09641a-4f7b-4e89-a603-39991d6317b4"
      },
      "source": [
        "index = LSHashing(data[\"vector\"], data[\"name\"])\n",
        "index.lsh_build()\n",
        "index.get_similar_tagged_questions(data[\"vector\"][0:2])"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question_id:0': ['bayesian', 'prior', 'elicitation'],\n",
              " 'question_id:19616': ['bayesian',\n",
              "  'machine-learning',\n",
              "  'nonparametric',\n",
              "  'stochastic-processes',\n",
              "  'nonparametric-bayes'],\n",
              " 'question_id:29362': ['bayesian',\n",
              "  'standard-deviation',\n",
              "  'r',\n",
              "  'estimation',\n",
              "  'robust'],\n",
              " 'question_id:35465': ['bayesian',\n",
              "  'summary-statistics',\n",
              "  'confidence-interval',\n",
              "  'posterior'],\n",
              " 'question_id:36333': ['bayesian',\n",
              "  'machine-learning',\n",
              "  'regression',\n",
              "  'logistic',\n",
              "  'bayes'],\n",
              " 'question_id:36871': ['bayesian',\n",
              "  'references',\n",
              "  'self-study',\n",
              "  'nonparametric-bayes'],\n",
              " 'question_id:37068': ['bayesian', 'machine-learning', 'regression'],\n",
              " 'question_id:37689': ['bayesian', 'correlation', 'data-visualization'],\n",
              " 'question_id:39589': ['bayesian',\n",
              "  'hypothesis-testing',\n",
              "  'proportion',\n",
              "  'ab-test'],\n",
              " 'question_id:43811': ['bayesian',\n",
              "  'variance',\n",
              "  'normal-distribution',\n",
              "  'variability'],\n",
              " 'question_id:45996': ['bayesian',\n",
              "  'prior',\n",
              "  'marginal',\n",
              "  'conditioning',\n",
              "  'prediction'],\n",
              " 'question_id:57972': ['bayesian', 'terminology']}"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUUjyeefJzw0"
      },
      "source": [
        "Product quantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnLclKnUJ6Vo"
      },
      "source": [
        "class PQRO():\n",
        "    def __init__(self, vectors, labels):\n",
        "        self.dimention = vectors.shape[1]\n",
        "        self.vectors = vectors.astype('float32')\n",
        "        self.labels = labels\n",
        "\n",
        "\n",
        "    def prod_quant_build(self, number_of_partition=8, search_in_x_partitions=2, subvector_size=8):\n",
        "        quantizer = faiss.IndexFlatL2(self.dimention)\n",
        "        self.index = faiss.IndexIVFPQ(quantizer, \n",
        "                                      self.dimention, \n",
        "                                      number_of_partition, \n",
        "                                      search_in_x_partitions, \n",
        "                                      subvector_size)\n",
        "        self.index.train(self.vectors)\n",
        "        self.index.add(self.vectors)    \n",
        "    def get_similar_tagged_questions(self, vectors, k=12):\n",
        "        distances, indices = self.index.search(vectors, k) \n",
        "        similar_q_w_feature={}\n",
        "        q_features=[]\n",
        "        for i in indices[0]:\n",
        "          similar_q_w_feature[self.labels[i]]=using_coo(i)\n",
        "        return similar_q_w_feature"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3h1THv7KkyT"
      },
      "source": [
        "Product quantization is an effective vector quantization\n",
        "approach to compactly encode high-dimensional vectors\n",
        "for fast approximate nearest neighbor (ANN) search.  The\n",
        "essence of product quantization is to decompose the original high-dimensional space into the Cartesian product of\n",
        "a finite number of low-dimensional subspaces that are then\n",
        "quantized separately. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYPli6OaKm_e",
        "outputId": "97bff611-025d-4104-e3b2-33dfe277bc9b"
      },
      "source": [
        "index = PQRO(data[\"vector\"], data[\"name\"])\n",
        "index.prod_quant_build()\n",
        "index.get_similar_tagged_questions(data[\"vector\"][0:2])"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question_id:0': ['bayesian', 'prior', 'elicitation'],\n",
              " 'question_id:19616': ['bayesian',\n",
              "  'machine-learning',\n",
              "  'nonparametric',\n",
              "  'stochastic-processes',\n",
              "  'nonparametric-bayes'],\n",
              " 'question_id:29362': ['bayesian',\n",
              "  'standard-deviation',\n",
              "  'r',\n",
              "  'estimation',\n",
              "  'robust'],\n",
              " 'question_id:35465': ['bayesian',\n",
              "  'summary-statistics',\n",
              "  'confidence-interval',\n",
              "  'posterior'],\n",
              " 'question_id:36333': ['bayesian',\n",
              "  'machine-learning',\n",
              "  'regression',\n",
              "  'logistic',\n",
              "  'bayes'],\n",
              " 'question_id:36871': ['bayesian',\n",
              "  'references',\n",
              "  'self-study',\n",
              "  'nonparametric-bayes'],\n",
              " 'question_id:37068': ['bayesian', 'machine-learning', 'regression'],\n",
              " 'question_id:37689': ['bayesian', 'correlation', 'data-visualization'],\n",
              " 'question_id:39589': ['bayesian',\n",
              "  'hypothesis-testing',\n",
              "  'proportion',\n",
              "  'ab-test'],\n",
              " 'question_id:43811': ['bayesian',\n",
              "  'variance',\n",
              "  'normal-distribution',\n",
              "  'variability'],\n",
              " 'question_id:45996': ['bayesian',\n",
              "  'prior',\n",
              "  'marginal',\n",
              "  'conditioning',\n",
              "  'prediction'],\n",
              " 'question_id:57972': ['bayesian', 'terminology']}"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxKNuT4lLWJB"
      },
      "source": [
        "HNSW (Hierarchical Navigable Small Worlds)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioNsx4x0LXfX"
      },
      "source": [
        "class HNSW():\n",
        "    def __init__(self, vectors, labels):\n",
        "        self.dimention = vectors.shape[1]\n",
        "        self.vectors = vectors.astype('float32')\n",
        "        self.labels = labels\n",
        "\n",
        "    def hnsw_build(self):\n",
        "        self.index = nmslib.init(method='hnsw', space='cosinesimil')\n",
        "        self.index.addDataPointBatch(self.vectors)\n",
        "        self.index.createIndex({'post': 2})\n",
        "    \n",
        "    def get_similar_tagged_questions(self, vector, k=12):\n",
        "        indices = self.index.knnQuery(vector, k=k)\n",
        "        similar_q_w_feature={}\n",
        "        q_features=[]\n",
        "        for i in indices[0]:\n",
        "          similar_q_w_feature[self.labels[i]]=using_coo(i)\n",
        "        return similar_q_w_feature"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2jk_PnyL4kr"
      },
      "source": [
        "Hierarchical Navigable Small World (HNSW) graphs are among the top-performing indexes for vector similarity search. HNSW is a hugely popular technology that time and time again produces state-of-the-art performance with super fast search speeds and fantastic recall."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpohAU-mL6wP",
        "outputId": "de86a3de-4060-4480-ee77-b9bc9d2138f5"
      },
      "source": [
        "index = HNSW(data[\"vector\"], data[\"name\"])\n",
        "index.hnsw_build()\n",
        "index.get_similar_tagged_questions(data[\"vector\"][2])"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question_id:151': ['open-source', 'books'],\n",
              " 'question_id:2': ['software', 'open-source'],\n",
              " 'question_id:205': ['software', 'modeling', 'real-time'],\n",
              " 'question_id:340': ['data-mining'],\n",
              " 'question_id:447': ['open-source'],\n",
              " 'question_id:46': ['r'],\n",
              " 'question_id:62': ['data-mining'],\n",
              " 'question_id:64': ['open-source', 'data-visualization'],\n",
              " 'question_id:729': ['data-visualization'],\n",
              " 'question_id:762': ['teaching'],\n",
              " 'question_id:7931': ['software', 'open-source', 'teaching'],\n",
              " 'question_id:948': ['software']}"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKwitjYwMUdC"
      },
      "source": [
        "Trees and graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vn_CkOpKMZvu"
      },
      "source": [
        "class TG():\n",
        "    def __init__(self, vectors, labels):\n",
        "        self.dimention = vectors.shape[1]\n",
        "        self.vectors = vectors.astype('float32')\n",
        "        self.labels = labels\n",
        "\n",
        "\n",
        "    def tree_build(self, number_of_trees=4):\n",
        "        self.index = annoy.AnnoyIndex(self.dimention)\n",
        "        for i, vec in enumerate(self.vectors):\n",
        "            self.index.add_item(i, vec.tolist())\n",
        "        self.index.build(number_of_trees)\n",
        "  \n",
        "    def get_similar_tagged_questions(self, vector, k=12):\n",
        "        indices = self.index.get_nns_by_vector(vector.tolist(), k)\n",
        "        similar_q_w_feature={}\n",
        "        q_features=[]\n",
        "        for i in indices:\n",
        "          similar_q_w_feature[self.labels[i]]=using_coo(i)\n",
        "        return similar_q_w_feature\n",
        "       "
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD4haGifNGT8"
      },
      "source": [
        "Propose an index structure, the ANN-tree/graphs (approximate nearest neighbor tree) to solve this problem. The ANN-tree supports high accuracy nearest neighbor search. The actual nearest neighbor of a query point can usually be found in the first leaf page accessed. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYVd8oJmNW1d",
        "outputId": "45f9818f-154e-41bd-9c2a-416e57b85b63"
      },
      "source": [
        "import annoy\n",
        "index = TG(data[\"vector\"], data[\"name\"])\n",
        "index.tree_build()\n",
        "index.get_similar_tagged_questions(data[\"vector\"][0])"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: FutureWarning: The default argument for metric will be removed in future version of Annoy. Please pass metric='angular' explicitly.\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question_id:0': ['bayesian', 'prior', 'elicitation'],\n",
              " 'question_id:14139': ['bayesian', 'hypothesis-testing', 'self-study'],\n",
              " 'question_id:20253': ['bayesian'],\n",
              " 'question_id:20474': ['bayesian'],\n",
              " 'question_id:34221': ['bayesian',\n",
              "  'prior',\n",
              "  'distributions',\n",
              "  'central-limit-theorem'],\n",
              " 'question_id:36078': ['bayesian'],\n",
              " 'question_id:36520': ['bayesian'],\n",
              " 'question_id:36713': ['bayesian', 'frequentist'],\n",
              " 'question_id:41860': ['bayesian'],\n",
              " 'question_id:53430': ['bayesian', 'prior'],\n",
              " 'question_id:60949': ['bayesian', 'prior'],\n",
              " 'question_id:69684': ['bayesian']}"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjWcr_wVN02P"
      },
      "source": [
        "# Summary\n",
        "Based on above  HNSW(NMSLib) is by far and away the best choice here. Trees and Graphs’s(annoy) performance is the worst at this particular task, it performs much better with cosine based lookup (like when computing similar items). Also, the indices are all memory mapped from file, which makes it much more suitable if you have multiple python processes serving up requests.\n",
        "\n",
        "\n"
      ]
    }
  ]
}