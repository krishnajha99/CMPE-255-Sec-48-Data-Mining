# -*- coding: utf-8 -*-
"""State_Of_Art_Approximate_Nearest_Neighbor_Search.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11xKQJy87Q7L8GneDDBz1QHicp7yw0GV2

# State of art libraries for Approximate nearest neighbor search for my favorite LightFM data set. 
The LightFM's built-in Dataset class to build an interaction dataset from raw data.  https://archive.org/details/stackexchange

The goal is to demonstrate how to use different ANN Search works 
* LSH
* Exhaustive search
* Product quantization
* Trees and graphs
* HNSW

Data Set Dependencies:
* Annoy 
* NMSLIB
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import numpy as np
import pandas as pd
from sklearn.metrics import pairwise_distances
import time
!pip install lightFM
from lightfm.datasets import fetch_stackexchange

#movielens = fetch_movielens()
data = fetch_stackexchange('crossvalidated',
                           test_set_fraction=0.1,
                           indicator_features=False,
                           tag_features=True)

for key, value in data.items():
    print(key, type(value), value.shape)

stackexchange = fetch_stackexchange('crossvalidated',indicator_features=True,tag_features=True)
train = data['train']
test = data['test']

# Commented out IPython magic to ensure Python compatibility.
from lightfm import LightFM

# Set the number of threads; you can increase this
# ify you have more physical cores available.
NUM_THREADS = 2
NUM_COMPONENTS = 30
NUM_EPOCHS = 3
ITEM_ALPHA = 1e-6

# Let's fit a WARP model: these generally have the best performance.
model = LightFM(loss='warp',
                item_alpha=ITEM_ALPHA,
               no_components=NUM_COMPONENTS)

# Run 3 epochs and time it.
# %time model = model.fit(train, epochs=NUM_EPOCHS, num_threads=NUM_THREADS)

"""As a means of sanity checking, let's calculate the model's AUC on the training set first. If it's reasonably high, we can be sure that the model is not doing anything stupid and is fitting the training data well."""

# Import the evaluation routines
from lightfm.evaluation import auc_score

# Compute and print the AUC score
train_auc = auc_score(model, train, num_threads=NUM_THREADS).mean()
print('Collaborative filtering train AUC: %s' % train_auc)

"""Fantastic, the model is fitting the training set well. But what about the test set.."""

# We pass in the train interactions to exclude them from predictions.
test_auc = auc_score(model, test, train_interactions=train, num_threads=NUM_THREADS).mean()
print('Collaborative filtering test AUC: %s' % test_auc)

""" Re-evaluating the model.


"""

# Set biases to zero
model.item_biases *= 0.0

test_auc = auc_score(model, test, train_interactions=train, num_threads=NUM_THREADS).mean()
print('Collaborative filtering test AUC: %s' % test_auc)

!pip install nmslib
import nmslib

# initialize a new nmslib index, using a HNSW index on Cosine Similarity
nms_idx = nmslib.init(method='hnsw', space='cosinesimil')
nms_idx.addDataPointBatch(item_embeddings)
nms_idx.createIndex(print_progress=True)

model = LightFM(learning_rate=0.05, loss='warp', no_components=64, item_alpha=0.001)
model.fit_partial(train, item_features=stackexchange['item_features'], epochs=20 )

item_vectors = stackexchange['item_features'] * model.item_embeddings

"""# Print the Dataset stackexchange"""

print(stackexchange['item_features'])

"""Load pretrained models from a .pkl file"""

import pickle
with open('stackexchange.pickle', 'wb') as f:
    pickle.dump({"name": stackexchange['item_feature_labels'], "vector": item_vectors,"features":stackexchange['item_features']}, f)
print(stackexchange['item_feature_labels'])

import pickle
def load_data():
    with open('stackexchange.pickle', 'rb') as f:
        data = pickle.load(f)
    return data
data = load_data()

"""Approximate based on the word2vec model for the dataset"""

item_features = stackexchange['item_features']
tag_labels = stackexchange['item_feature_labels']

print('There are %s distinct tags, with values like %s.' % (item_features.shape[1], tag_labels[:3].tolist()))
# Define a new model instance
model = LightFM(loss='warp',
                item_alpha=ITEM_ALPHA,
                no_components=NUM_COMPONENTS)

# Fit the hybrid model. Note that this time, we pass
# in the item features matrix.
model = model.fit(train,
                item_features=item_features,
                epochs=NUM_EPOCHS,
                num_threads=NUM_THREADS)

tag_labels = stackexchange['item_feature_labels']
def get_similar_tags(model, tag_id):
    # Define similarity as the cosine of the angle
    # between the tag latent vectors
    
    # Normalize the vectors to unit length
    tag_embeddings = (model.item_embeddings.T
                      / np.linalg.norm(model.item_embeddings, axis=1)).T
    
    query_embedding = tag_embeddings[tag_id]
    similarity = np.dot(tag_embeddings, query_embedding)
    most_similar = np.argsort(-similarity)[1:4]
    
    return most_similar


for tag in (u'bayesian', u'regression', u'survival'):
    tag_id = tag_labels.tolist().index(tag)
    print('Most similar tags for %s: %s' % (tag_labels[tag_id],
                                            tag_labels[get_similar_tags(model, tag_id)]))

"""Exhaustive search"""

import scipy.sparse
import random
import itertools
cx = scipy.sparse.coo_matrix(stackexchange['item_features'])
def using_coo(index):
    arr=[]
       
    for i,j,v in zip(cx.row, cx.col, cx.data):
      if i==index and i != j:
        arr.append(data['name'][j])
      elif i>index:
        break
    return arr
      # print(j,data['name'][j])
using_coo(0)

class exSearch():
    def __init__(self, vectors, labels):
        self.vectors = vectors.astype('float32')
        self.labels = labels
        self.index = faiss.IndexFlatL2(vectors.shape[1])
        self.index.add(self.vectors)
    def get_similar_tagged_questions(self,vectors,k=12):
        distances, indices = self.index.search(vectors, k) 
        similar_q_w_feature={}
        q_features=[]
        for i in indices[0]:
          similar_q_w_feature[self.labels[i]]=using_coo(i)
        return similar_q_w_feature

# Exhaustive search for brute-force approach to combinatorial finding similar item in dataset

!pip install faiss
!apt install libomp-dev
!python3 -m pip install --upgrade faiss-gpu==1.7.1
import faiss
index = exSearch(data["vector"], data["name"])
index.get_similar_tagged_questions(data['vector'][0:4])

"""LSH"""

#LSH (Locality sensitive Hashing), an approach that enable us to search for approximate nearest neighbors for a given point in metric space
class LSHashing():
    def __init__(self, vectors, labels):
        self.dimention = vectors.shape[1]
        self.vectors = vectors.astype('float32')
        self.labels = labels


    def lsh_build(self, number_of_partition=8, search_in_x_partitions=2, subvector_size=8):
        quantizer = faiss.IndexFlatL2(self.dimention)
        self.index = faiss.IndexIVFPQ(quantizer, self.dimention, number_of_partition, search_in_x_partitions, subvector_size)
        self.index.train(self.vectors)
        self.index.add(self.vectors)

    def get_similar_tagged_questions(self, vectors, k=12):
        distances, indices = self.index.search(vectors, k) 
        similar_q_w_feature={}
        q_features=[]
        for i in indices[0]:
          similar_q_w_feature[self.labels[i]]=using_coo(i)
        return similar_q_w_feature

index = LSHashing(data["vector"], data["name"])
index.lsh_build()
index.get_similar_tagged_questions(data["vector"][0:2])

"""Product quantization"""

class PQRO():
    def __init__(self, vectors, labels):
        self.dimention = vectors.shape[1]
        self.vectors = vectors.astype('float32')
        self.labels = labels


    def prod_quant_build(self, number_of_partition=8, search_in_x_partitions=2, subvector_size=8):
        quantizer = faiss.IndexFlatL2(self.dimention)
        self.index = faiss.IndexIVFPQ(quantizer, 
                                      self.dimention, 
                                      number_of_partition, 
                                      search_in_x_partitions, 
                                      subvector_size)
        self.index.train(self.vectors)
        self.index.add(self.vectors)    
    def get_similar_tagged_questions(self, vectors, k=12):
        distances, indices = self.index.search(vectors, k) 
        similar_q_w_feature={}
        q_features=[]
        for i in indices[0]:
          similar_q_w_feature[self.labels[i]]=using_coo(i)
        return similar_q_w_feature

"""Product quantization is an effective vector quantization
approach to compactly encode high-dimensional vectors
for fast approximate nearest neighbor (ANN) search.  The
essence of product quantization is to decompose the original high-dimensional space into the Cartesian product of
a finite number of low-dimensional subspaces that are then
quantized separately. 
"""

index = PQRO(data["vector"], data["name"])
index.prod_quant_build()
index.get_similar_tagged_questions(data["vector"][0:2])

"""HNSW (Hierarchical Navigable Small Worlds)"""

class HNSW():
    def __init__(self, vectors, labels):
        self.dimention = vectors.shape[1]
        self.vectors = vectors.astype('float32')
        self.labels = labels

    def hnsw_build(self):
        self.index = nmslib.init(method='hnsw', space='cosinesimil')
        self.index.addDataPointBatch(self.vectors)
        self.index.createIndex({'post': 2})
    
    def get_similar_tagged_questions(self, vector, k=12):
        indices = self.index.knnQuery(vector, k=k)
        similar_q_w_feature={}
        q_features=[]
        for i in indices[0]:
          similar_q_w_feature[self.labels[i]]=using_coo(i)
        return similar_q_w_feature

"""Hierarchical Navigable Small World (HNSW) graphs are among the top-performing indexes for vector similarity search. HNSW is a hugely popular technology that time and time again produces state-of-the-art performance with super fast search speeds and fantastic recall."""

index = HNSW(data["vector"], data["name"])
index.hnsw_build()
index.get_similar_tagged_questions(data["vector"][2])

"""Trees and graphs"""

class TG():
    def __init__(self, vectors, labels):
        self.dimention = vectors.shape[1]
        self.vectors = vectors.astype('float32')
        self.labels = labels


    def tree_build(self, number_of_trees=4):
        self.index = annoy.AnnoyIndex(self.dimention)
        for i, vec in enumerate(self.vectors):
            self.index.add_item(i, vec.tolist())
        self.index.build(number_of_trees)
  
    def get_similar_tagged_questions(self, vector, k=12):
        indices = self.index.get_nns_by_vector(vector.tolist(), k)
        similar_q_w_feature={}
        q_features=[]
        for i in indices:
          similar_q_w_feature[self.labels[i]]=using_coo(i)
        return similar_q_w_feature

"""Propose an index structure, the ANN-tree/graphs (approximate nearest neighbor tree) to solve this problem. The ANN-tree supports high accuracy nearest neighbor search. The actual nearest neighbor of a query point can usually be found in the first leaf page accessed. 


"""

import annoy
index = TG(data["vector"], data["name"])
index.tree_build()
index.get_similar_tagged_questions(data["vector"][0])

"""# Summary
Based on above  HNSW(NMSLib) is by far and away the best choice here. Trees and Graphs’s(annoy) performance is the worst at this particular task, it performs much better with cosine based lookup (like when computing similar items). Also, the indices are all memory mapped from file, which makes it much more suitable if you have multiple python processes serving up requests.



"""